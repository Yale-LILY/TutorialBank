in the index 


<div id="container" class="site-main">
-1
null

-1
null
<div class="butterBar butterBar--error">
-1
null

-1
null
</div>
-1
null

-1
null
<div class="surface">
-1
null

-1
null
<div id="prerendered" class="screenContent">
-1
null

-1
null
<div
-1
null
class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer">
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="metabar u-clearfix js-metabar u-fixed u-backgroundTransparentWhiteDarkest u-xs-sizeFullViewportWidth">
-1
null

-1
null
<div
-1
null
class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1000 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingLeft20 u-paddingRight20">
-1
null

-1
null
<div class="metabar-block u-flex1 u-flexCenter">
-1
null

-1
null
<div class="js-metabarLogoLeft u-xs-show">
-1
null

-1
null
[Homepage]{.u-textScreenReader}
-1
null

-1
null
</div>
-1
null

-1
null
[About
-1
null
membership](https://medium.com/membership?source=upgrade_membership---nav_full){.link
-1
null
.link--darken .u-accentColor--textDarken .u-baseColor--link .u-xs-hide
-1
null
.js-upgradeMembershipAction}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="metabar-block u-flex0">
-1
null

-1
null
<div class="buttonSet buttonSet--wide">
-1
null

-1
null
[Sign
-1
null
in](https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2F%40karpathy%2Fyes-you-should-understand-backprop-e2f06eab496b&source=--------------------------nav_reg&operation=login){.button
-1
null
.button--primary .button--chromeless .u-accentColor--buttonNormal
-1
null
.is-inSiteNavBar .u-xs-hide .js-signInButton}[Get
-1
null
started](https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2F%40karpathy%2Fyes-you-should-understand-backprop-e2f06eab496b&source=--------------------------nav_reg&operation=register){.button
-1
null
.button--primary .button--withChrome .u-accentColor--buttonNormal
-1
null
.is-inSiteNavBar .js-signUpButton}
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="u-absolute u-sizeFullWidth u-top0 u-right0 u-bottom0 u-flexCenter u-justifyContentCenter u-textAlignCenter u-xs-hide js-metabarLogoCentered">
-1
null

-1
null
[Homepage]{.u-textScreenReader}
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56">
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaHeader u-paddingBottom10 row">
-1
null

-1
null
<div class="col u-size12of12 js-postMetaLockup">
-1
null

-1
null
<div
-1
null
class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaLockup postMetaLockup--authorWithBio u-flexCenter js-postMetaLockup">
-1
null

-1
null
<div class="u-flex0">
-1
null

-1
null
[![Go to the profile of Andrej
-1
null
Karpathy](https://cdn-images-1.medium.com/fit/c/120/120/0*8ldFdx9B6FhSkQmV.jpeg){.avatar-image
-1
null
.avatar-image--small}](https://medium.com/@karpathy?source=post_header_lockup){.link
-1
null
.u-baseColor--link .avatar}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-flex1 u-paddingLeft15 u-overflowHidden">
-1
null

-1
null
<div class="u-lineHeightTightest">
-1
null

-1
null
[Andrej
-1
null
Karpathy](https://medium.com/@karpathy?source=post_header_lockup){.ds-link
-1
null
.ds-link--styleSubtle .ui-captionStrong .u-inlineBlock .link
-1
null
.link--darken .link--darker}[]{.followState .js-followState
-1
null
data-user-id="ac9d9a35533e"}
-1
null
[Blocked]{.button-label .button-defaultState}[Unblock]{.button-label
-1
null
.button-hoverState}
-1
null
[Follow]{.button-label .button-defaultState
-1
null
.js-buttonLabel}[Following]{.button-label .button-activeState}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="ui-caption ui-xs-clamp2 postMetaInline">
-1
null

-1
null
Director of AI at Tesla. Previously Research Scientist at OpenAI and PhD
-1
null
student at Stanford. I like to train deep neural nets on large datasets.
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="ui-caption postMetaInline js-testPostMetaInlineSupplemental">
-1
null

-1
null
Dec 19, 2016[]{.middotDivider .u-fontSize12}[]{.readingTime
-1
null
title="7 min read"}
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="postArticle-content js-postField js-notesSource js-trackedPost"
-1
null
data-post-id="e2f06eab496b" data-source="post_page"
-1
null
data-tracking-context="postPage">
-1
null

-1
null
<div class="section section section--body section--first section--last"
-1
null
name="52e8">
-1
null

-1
null
<div class="section-divider">
-1
null

-1
null
------------------------------------------------------------------------
-1
null

-1
null
</div>
-1
null

-1
null
<div class="section-content">
-1
null

-1
null
<div class="section-inner sectionLayout--insetColumn">
-1
null

-1
null
Yes you should understand backprop {#bc38 .graf .graf--h3 .graf--leading .graf--title name="bc38"}
-1
null
==================================
-1
null

-1
null
When we offered [CS231n](http://cs231n.stanford.edu/){.markup--anchor
-1
null
.markup--p-anchor} (Deep Learning class) at Stanford, we intentionally
-1
null
designed the programming assignments to include explicit calculations
-1
null
involved in backpropagation on the lowest level. The students had to
-1
null
implement the forward and the backward pass of each layer in raw numpy.
-1
null
Inevitably, some students complained on the class message boards:
-1
null

-1
null
> “Why do we have to write the backward pass when frameworks in the real
-1
null
> world, such as TensorFlow, compute them for you automatically?”
-1
null

-1
null
This is seemingly a perfectly sensible appeal - if you’re never going to
-1
null
write backward passes once the class is over, why practice writing them?
-1
null
Are we just torturing the students for our own amusement? Some easy
-1
null
answers could make arguments along the lines of *“it’s worth knowing
-1
null
what’s under the hood as an intellectual curiosity”*, or perhaps *“you
-1
null
might want to improve on the core algorithm later”*, but there is a much
-1
null
stronger and practical argument, which I wanted to devote a whole post
-1
null
to:
-1
null

-1
null
**&gt; The problem with Backpropagation is that it is a** [**leaky
-1
null
abstraction**](https://en.wikipedia.org/wiki/Leaky_abstraction){.markup--anchor
-1
null
.markup--p-anchor}**.**
-1
null

-1
null
In other words, it is easy to fall into the trap of abstracting away the
-1
null
learning process — believing that you can simply stack arbitrary layers
-1
null
together and backprop will “magically make them work” on your data. So
-1
null
lets look at a few explicit examples where this is not the case in quite
-1
null
unintuitive ways.
-1
null

-1
null
<div class="aspectRatioPlaceholder is-locked"
-1
null
style="max-width: 700px; max-height: 193px;">
-1
null

-1
null
<div class="aspectRatioPlaceholder-fill"
-1
null
style="padding-bottom: 27.500000000000004%;">
-1
null

-1
null
</div>
-1
null

-1
null
![](https://cdn-images-1.medium.com/max/1600/1*Ms0ggCGJ2gZqJJlY16wQ4w.png){.graf-image}
-1
[ '![](https://cdn-images-1.medium.com/max/1600/1*Ms0ggCGJ2gZqJJlY16wQ4w.png',
  index: 0,
  input: '![](https://cdn-images-1.medium.com/max/1600/1*Ms0ggCGJ2gZqJJlY16wQ4w.png){.graf-image}' ]

-1
null
</div>
-1
null

-1
null
Some eye candy: a computational graph of a Batch Norm layer with a
-1
null
forward pass (black) and backward pass (red). (borrowed from
-1
null
[this post](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html){.markup--anchor
-1
null
.markup--figure-anchor})
-1
null
#### Vanishing gradients on sigmoids {#4c13 .graf .graf--h4 .graf-after--figure name="4c13"}
-1
null

-1
null
We’re starting off easy here. At one point it was fashionable to use
-1
null
**sigmoid** (or **tanh**) non-linearities in the fully connected layers.
-1
null
The tricky part people might not realize until they think about the
-1
null
backward pass is that if you are sloppy with the weight initialization
-1
null
or data preprocessing these non-linearities can “saturate” and entirely
-1
null
stop learning — your training loss will be flat and refuse to go down.
-1
null
For example, a fully connected layer with sigmoid non-linearity computes
-1
null
(using raw numpy):
-1
null

-1
null
``` {#188e .graf .graf--pre .graf-after--p name="188e"}
-1
null
z = 1/(1 + np.exp(-np.dot(W, x))) # forward pass
-1
null
dx = np.dot(W.T, z*(1-z)) # backward pass: local gradient for x
-1
null
dW = np.outer(z*(1-z), x) # backward pass: local gradient for W
-1
null
```
-1
null

-1
null
If your weight matrix **W** is initialized too large, the output of the
-1
null
matrix multiply could have a very large range (e.g. numbers between -400
-1
null
and 400), which will make all outputs in the vector **z** almost binary:
-1
null
either 1 or 0. But if that is the case, **z\*(1-z)**, which is local
-1
null
gradient of the sigmoid non-linearity, will in both cases become
-1
null
**zero** (“vanish”)**,** making the gradient for both **x** and **W** be
-1
null
zero. The rest of the backward pass will come out all zero from this
-1
null
point on due to multiplication in the chain rule.
-1
null

-1
null
<div class="aspectRatioPlaceholder is-locked"
-1
null
style="max-width: 700px; max-height: 244px;">
-1
null

-1
null
<div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.8%;">
-1
null

-1
null
</div>
-1
null

-1
null
![](https://cdn-images-1.medium.com/max/1600/1*gkXI7LYwyGPLU5dn6Jb6Bg.png){.graf-image}
-1
[ '![](https://cdn-images-1.medium.com/max/1600/1*gkXI7LYwyGPLU5dn6Jb6Bg.png',
  index: 0,
  input: '![](https://cdn-images-1.medium.com/max/1600/1*gkXI7LYwyGPLU5dn6Jb6Bg.png){.graf-image}' ]

-1
null
</div>
-1
null

-1
null
Another non-obvious fun fact about sigmoid is that its local gradient
-1
null
(z\*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that
-1
null
every time the gradient signal flows through a sigmoid gate, its
-1
null
magnitude always diminishes by one quarter (or more). If you’re using
-1
null
basic SGD, this would make the lower layers of a network train much
-1
null
slower than the higher ones.
-1
null

-1
null
**TLDR**: if you’re using **sigmoids** or **tanh** non-linearities in
-1
null
your network and you understand backpropagation you should always be
-1
null
nervous about making sure that the initialization doesn’t cause them to
-1
null
be fully saturated. See a longer explanation in this [CS231n lecture
-1
null
video](https://youtu.be/gYpoJMlgyXA?t=14m14s){.markup--anchor
-1
null
.markup--p-anchor}.
-1
null

-1
null
#### Dying ReLUs {#d1ad .graf .graf--h4 .graf-after--p name="d1ad"}
-1
null

-1
null
Another fun non-linearity is the ReLU, which thresholds neurons at zero
-1
null
from below. The forward and backward pass for a fully connected layer
-1
null
that uses ReLU would at the core include:
-1
null

-1
null
``` {#d710 .graf .graf--pre .graf-after--p name="d710"}
-1
null
z = np.maximum(0, np.dot(W, x)) # forward pass
-1
null
dW = np.outer(z > 0, x) # backward pass: local gradient for W
-1
null
```
-1
null

-1
null
If you stare at this for a while you’ll see that if a neuron gets
-1
null
clamped to zero in the forward pass (i.e. **z**=0, it doesn’t “fire”),
-1
null
then its weights will get zero gradient. This can lead to what is called
-1
null
the “dead ReLU” problem, where if a ReLU neuron is unfortunately
-1
null
initialized such that it never fires, or if a neuron’s weights ever get
-1
null
knocked off with a large update during training into this regime, then
-1
null
this neuron will remain permanently dead. It’s like permanent,
-1
null
irrecoverable brain damage. Sometimes you can forward the entire
-1
null
training set through a trained network and find that a large fraction
-1
null
(e.g. 40%) of your neurons were zero the entire time.
-1
null

-1
null
<div class="aspectRatioPlaceholder is-locked"
-1
null
style="max-width: 700px; max-height: 242px;">
-1
null

-1
null
<div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.5%;">
-1
null

-1
null
</div>
-1
null

-1
null
![](https://cdn-images-1.medium.com/max/1600/1*g0yxlK8kEBw8uA1f82XQdA.png){.graf-image}
-1
[ '![](https://cdn-images-1.medium.com/max/1600/1*g0yxlK8kEBw8uA1f82XQdA.png',
  index: 0,
  input: '![](https://cdn-images-1.medium.com/max/1600/1*g0yxlK8kEBw8uA1f82XQdA.png){.graf-image}' ]

-1
null
</div>
-1
null

-1
null
**TLDR**: If you understand backpropagation and your network has ReLUs,
-1
null
you’re always nervous about dead ReLUs. These are neurons that never
-1
null
turn on for any example in your entire training set, and will remain
-1
null
permanently dead. Neurons can also die during training, usually as a
-1
null
symptom of aggressive learning rates. See a longer explanation in
-1
null
[CS231n lecture
-1
null
video](https://youtu.be/gYpoJMlgyXA?t=20m54s){.markup--anchor
-1
null
.markup--p-anchor}.
-1
null

-1
null
#### Exploding gradients in RNNs {#60f1 .graf .graf--h4 .graf-after--p name="60f1"}
-1
null

-1
null
Vanilla RNNs feature another good example of unintuitive effects of
-1
null
backpropagation. I’ll copy paste a slide from CS231n that has a
-1
null
simplified RNN that does not take any input **x**, and only computes the
-1
null
recurrence on the hidden state (equivalently, the input **x** could
-1
null
always be zero):
-1
null

-1
null
<div class="aspectRatioPlaceholder is-locked"
-1
null
style="max-width: 700px; max-height: 256px;">
-1
null

-1
null
<div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.5%;">
-1
null

-1
null
</div>
-1
null

-1
null
![](https://cdn-images-1.medium.com/max/1600/1*dqlX0ixpk1O3225bZ1LGnA.png){.graf-image}
-1
[ '![](https://cdn-images-1.medium.com/max/1600/1*dqlX0ixpk1O3225bZ1LGnA.png',
  index: 0,
  input: '![](https://cdn-images-1.medium.com/max/1600/1*dqlX0ixpk1O3225bZ1LGnA.png){.graf-image}' ]

-1
null
</div>
-1
null

-1
null
This RNN is unrolled for **T** time steps. When you stare at what the
-1
null
backward pass is doing, you’ll see that the gradient signal going
-1
null
backwards in time through all the hidden states is always being
-1
null
multiplied by the same matrix (the recurrence matrix **Whh**),
-1
null
interspersed with non-linearity backprop.
-1
null

-1
null
What happens when you take one number **a** and start multiplying it by
-1
null
some other number **b** (i.e. a\*b\*b\*b\*b\*b\*b…)? This sequence
-1
null
either goes to zero if **|b|** &lt; 1, or explodes to infinity when
-1
null
**|b|&gt;1**. The same thing happens in the backward pass of an RNN,
-1
null
except **b** is a matrix and not just a number, so we have to reason
-1
null
about its largest eigenvalue instead.
-1
null

-1
null
**TLDR**: If you understand backpropagation and you’re using RNNs you
-1
null
are nervous about having to do gradient clipping, or you prefer to use
-1
null
an LSTM. See a longer explanation in this [CS231n lecture
-1
null
video](https://www.youtube.com/watch?v=yCC09vCHzF8){.markup--anchor
-1
null
.markup--p-anchor}.
-1
null

-1
null
#### Spotted in the Wild: DQN Clipping {#a9a8 .graf .graf--h4 .graf-after--p name="a9a8"}
-1
null

-1
null
Lets look at one more — the one that actually inspired this post.
-1
null
Yesterday I was browsing for a Deep Q Learning implementation in
-1
null
TensorFlow (to see how others deal with computing the numpy equivalent
-1
null
of **Q\[:, a\]**, where **a** is an integer vector — turns out this
-1
null
trivial operation is not supported in TF). Anyway, I searched *“dqn
-1
null
tensorflow”*, clicked the first link, and found the core code. Here is
-1
null
an excerpt:
-1
null

-1
null
<div class="aspectRatioPlaceholder is-locked"
-1
null
style="max-width: 700px; max-height: 177px;">
-1
null

-1
null
<div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.3%;">
-1
null

-1
null
</div>
-1
null

-1
null
![](https://cdn-images-1.medium.com/max/1600/1*pyz5lHFDho07cFzIA6tWmQ.png){.graf-image}
-1
[ '![](https://cdn-images-1.medium.com/max/1600/1*pyz5lHFDho07cFzIA6tWmQ.png',
  index: 0,
  input: '![](https://cdn-images-1.medium.com/max/1600/1*pyz5lHFDho07cFzIA6tWmQ.png){.graf-image}' ]

-1
null
</div>
-1
null

-1
null
If you’re familiar with DQN, you can see that there is the
-1
null
**target\_q\_t,** which is just **\[reward \* \\gamma \\argmax\_a
-1
null
Q(s’,a)\]**, and then there is **q\_acted**, which is **Q(s,a)** of the
-1
null
action that was taken. The authors here subtract the two into variable
-1
null
**delta,** which they then want to minimize on line 295 with the L2 loss
-1
null
with **tf.reduce\_mean(tf.square()).** So far so good.
-1
null

-1
null
The problem is on line 291. The authors are trying to be robust to
-1
null
outliers, so if the delta is too large, they clip it with
-1
null
**tf.clip\_by\_value**. This is well-intentioned and looks sensible from
-1
null
the perspective of the forward pass, but it introduces a major bug if
-1
null
you think about the backward pass.
-1
null

-1
null
The **clip\_by\_value** function has a local gradient of zero outside of
-1
null
the range **min\_delta** to **max\_delta**, so whenever the delta is
-1
null
above min/max\_delta, the gradient becomes exactly zero during backprop.
-1
null
The authors are clipping the raw Q delta, when they are likely trying to
-1
null
clip the gradient for added robustness. In that case the correct thing
-1
null
to do is to use the Huber loss in place of **tf.square**:
-1
null

-1
null
``` {#a9c1 .graf .graf--pre .graf-after--p name="a9c1"}
-1
null
def clipped_error(x): 
-1
null
  return tf.select(tf.abs(x) < 1.0, 
-1
null
                   0.5 * tf.square(x), 
-1
null
                   tf.abs(x) - 0.5) # condition, true, false
-1
null
```
-1
null

-1
null
It’s a bit gross in TensorFlow because all we want to do is clip the
-1
null
gradient if it is above a threshold, but since we can’t meddle with the
-1
null
gradients directly we have to do it in this round-about way of defining
-1
null
the Huber loss. In Torch this would be much more simple.
-1
null

-1
null
I submitted an
-1
null
[issue](https://github.com/devsisters/DQN-tensorflow/issues/16){.markup--anchor
-1
null
.markup--p-anchor} on the DQN repo and this was promptly fixed.
-1
null

-1
null
#### In conclusion {#77e3 .graf .graf--h4 .graf-after--p name="77e3"}
-1
null

-1
null
Backpropagation is a leaky abstraction; it is a credit assignment scheme
-1
null
with non-trivial consequences. If you try to ignore how it works under
-1
null
the hood because “TensorFlow automagically makes my networks learn”, you
-1
null
will not be ready to wrestle with the dangers it presents, and you will
-1
null
be much less effective at building and debugging neural networks.
-1
null

-1
null
The good news is that backpropagation is not that difficult to
-1
null
understand, if presented properly. I have relatively strong feelings on
-1
null
this topic because it seems to me that 95% of backpropagation materials
-1
null
out there present it all wrong, filling pages with mechanical math.
-1
null
Instead, I would recommend the [CS231n lecture on
-1
null
backprop](https://www.youtube.com/watch?v=i94OvYb6noo){.markup--anchor
-1
null
.markup--p-anchor} which emphasizes intuition (yay for shameless
-1
null
self-advertising). And if you can spare the time, as a bonus, work
-1
null
through the [CS231n
-1
null
assignments](https://cs231n.github.io/){.markup--anchor
-1
null
.markup--p-anchor}, which get you to write backprop manually and help
-1
null
you solidify your understanding.
-1
null

-1
null
That’s it for now! I hope you’ll be much more suspicious of
-1
null
backpropagation going forward and think carefully through what the
-1
null
backward pass is doing. Also, I’m aware that this post has
-1
null
(unintentionally!) turned into several CS231n ads. Apologies for that :)
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="container u-maxWidth740">
-1
null

-1
null
<div class="row">
-1
null

-1
null
<div class="col u-size12of12">
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="row">
-1
null

-1
null
<div class="col u-size12of12 js-postTags">
-1
null

-1
null
<div class="u-paddingBottom10">
-1
null

-1
null
-   [Machine
-1
null
    Learning](https://medium.com/tag/machine-learning?source=post){.link
-1
null
    .u-baseColor--link}
-1
null
-   [Neural
-1
null
    Networks](https://medium.com/tag/neural-networks?source=post){.link
-1
null
    .u-baseColor--link}
-1
null
-   [Deep
-1
null
    Learning](https://medium.com/tag/deep-learning?source=post){.link
-1
null
    .u-baseColor--link}
-1
null
-   [Artificial
-1
null
    Intelligence](https://medium.com/tag/artificial-intelligence?source=post){.link
-1
null
    .u-baseColor--link}
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="section uiScale uiScale-ui--small uiScale-caption--regular u-borderTopLightest u-marginTop10 u-paddingTop20">
-1
null

-1
null
<div class="ui-h3 u-textColorDarker u-fontSize22">
-1
null

-1
null
One clap, two clap, three clap, forty?
-1
null

-1
null
</div>
-1
null

-1
null
By clapping more or less, you can signal to us which stories really
-1
null
stand out.
-1
null

-1
null
</div>
-1
null

-1
null
<div class="postActions js-postActionsFooter">
-1
null

-1
null
<div class="u-flexCenter">
-1
null

-1
null
<div class="u-flex1">
-1
null

-1
null
<div
-1
null
class="multirecommend js-actionMultirecommend u-flexCenter u-width60"
-1
null
data-post-id="e2f06eab496b" data-is-icon-29px="true"
-1
null
data-is-circle="true" data-has-recommend-list="true"
-1
null
data-source="post_actions_footer-----e2f06eab496b---------------------clap_footer">
-1
null

-1
null
<div class="u-relative u-foreground">
-1
null

-1
null
<div
-1
null
class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo"
-1
null
style="top: 14px; padding: 2px;">
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
[]{.u-textAlignCenter .u-relative .u-background
-1
null
.js-actionMultirecommendCount .u-marginLeft10}
-1
null
6.5K
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="buttonSet u-flex0">
-1
null

-1
null
36
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer">
-1
null

-1
null
<div class="row js-postFooterInfo">
-1
null

-1
null
<div class="col u-size12of12">
-1
null

-1
null
<div class="u-marginLeft20 u-floatRight">
-1
null

-1
null
[]{.followState .js-followState data-user-id="ac9d9a35533e"}
-1
null
[Blocked]{.button-label .button-defaultState}[Unblock]{.button-label
-1
null
.button-hoverState}
-1
null
[Follow]{.button-label .button-defaultState
-1
null
.js-buttonLabel}[Following]{.button-label .button-activeState}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-tableCell">
-1
null

-1
null
[![Go to the profile of Andrej
-1
null
Karpathy](https://cdn-images-1.medium.com/fit/c/120/120/0*8ldFdx9B6FhSkQmV.jpeg){.avatar-image
-1
null
.avatar-image--small}](https://medium.com/@karpathy?source=footer_card "Go to the profile of Andrej Karpathy"){.link
-1
null
.u-baseColor--link .avatar}
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15">
-1
null

-1
null
### [Andrej Karpathy](https://medium.com/@karpathy "Go to the profile of Andrej Karpathy"){.link .link--primary .u-accentColor--hoverTextNormal} {#andrej-karpathy .ui-h3 .u-fontSize18 .u-lineHeightTighter .u-marginBottom4}
-1
null

-1
null
Director of AI at Tesla. Previously Research Scientist at OpenAI and PhD
-1
null
student at Stanford. I like to train deep neural nets on large datasets.
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="js-postFooterPlacements">
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper">
-1
null

-1
null
</div>
-1
null

-1
null
<div class="supplementalPostContent js-heroPromo">
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-marginAuto u-maxWidth1000">
-1
null

-1
null
<div
-1
null
class="js-postShareWidget u-foreground u-sm-hide u-transition--fadeOut300 u-fixed">
-1
null

-1
null
-   <div
-1
null
    class="multirecommend js-actionMultirecommend u-flexColumn u-marginBottom10 u-width60"
-1
null
    data-post-id="e2f06eab496b" data-is-icon-29px="true"
-1
null
    data-is-vertical="true" data-is-circle="true"
-1
null
    data-has-recommend-list="true"
-1
null
    data-source="post_share_widget-----e2f06eab496b---------------------clap_sidebar">
-1
null

-1
null
    <div class="u-relative u-foreground">
-1
null

-1
null
    <div
-1
null
    class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo"
-1
null
    style="top: 14px; padding: 2px;">
-1
null

-1
null
    </div>
-1
null

-1
null
    </div>
-1
null

-1
null
    []{.u-textAlignCenter .u-relative .u-background
-1
null
    .js-actionMultirecommendCount .u-flexOrderNegative1 .u-height20
-1
null
    .u-marginBottom7}
-1
null
    6.5K
-1
null

-1
null
    </div>
-1
null

-1
null
-   -   -   
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div
-1
null
class="u-fixed u-bottom0 u-sizeFullWidth u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-paddingLeft10 u-xs-paddingRight10 js-stickyFooter">
-1
null

-1
null
<div class="u-maxWidth700 u-marginAuto u-flexCenter">
-1
null

-1
null
<div class="u-fontSize16 u-flex1 u-flexCenter">
-1
null

-1
null
<div class="u-flex0 u-inlineBlock u-paddingRight20 u-xs-paddingRight10">
-1
null

-1
null
[![Go to the profile of Andrej
-1
null
Karpathy](https://cdn-images-1.medium.com/fit/c/80/80/0*8ldFdx9B6FhSkQmV.jpeg){.avatar-image
-1
null
.avatar-image--smaller}](https://medium.com/@karpathy){.link
-1
null
.u-baseColor--link .avatar .u-inline}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-flex1 u-inlineBlock">
-1
null

-1
null
<div class="u-xs-hide">
-1
null

-1
null
Never miss a story from **Andrej Karpathy**, when you sign up for
-1
null
Medium. [Learn
-1
null
more](https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg){.link
-1
null
.u-baseColor--link .link--accent .u-accentColor--textNormal
-1
null
.u-accentColor--textDarken}
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-xs-show">
-1
null

-1
null
Never miss a story from **Andrej Karpathy**
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="u-marginLeft50 u-xs-marginAuto">
-1
null

-1
null
[]{.followState .js-followState data-user-id="ac9d9a35533e"}
-1
null
[Blocked]{.button-label .button-defaultState}[Unblock]{.button-label
-1
null
.button-hoverState}
-1
null
[Follow]{.button-label .button-defaultState .js-buttonLabel}[Get
-1
null
updates]{.button-label .button-activeState}
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
</div>
-1
null

-1
null
<div class="loadingBar">
-1
null

-1
null
</div>
-1
null

-1
null
